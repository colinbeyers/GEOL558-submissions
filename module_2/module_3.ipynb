{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Module 3:** Supervised Classification Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Colin Beyers\\\n",
    "GEOL558"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Title**: Classifying Sea State with Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project aims to classify sea states based on oceanographic variables such as significant wave height, wave period, and wind speed. Sea states are crucial for maritime activities, providing valuable information on potential hazards and navigational conditions. Machine learning (ML) is employed to automate the classification process, leveraging large datasets and complex relationships between environmental factors that are difficult to model explicitly using traditional approaches. The project uses observational data from a NOAA National Data Buoy Center (NDBC) buoy 46025, off the coast of Malibu. This buoy measures significant wave height (WVHT), wave period (DPD), and wind speed (WSPD), which are key predictors of sea state conditions. By applying ML, the project can efficiently analyze and classify sea state conditions in real-time, helping decision-makers in shipping, navigation, and coastal management."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install cartopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data handling\n",
    "import pandas as pd\n",
    "\n",
    "# plotting \n",
    "import seaborn              as sbn\n",
    "import matplotlib.pyplot    as pyp\n",
    "import cartopy.crs          as ccrs\n",
    "import cartopy.feature      as cfeature\n",
    "\n",
    "# modeling\n",
    "from sklearn.preprocessing      import StandardScaler, LabelEncoder\n",
    "from sklearn.ensemble           import GradientBoostingClassifier\n",
    "from sklearn.linear_model       import LogisticRegression\n",
    "from sklearn.svm                import SVC\n",
    "from sklearn.model_selection    import GridSearchCV\n",
    "from sklearn.metrics            import precision_score, recall_score, f1_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Aquisition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NDBC data can be easily accessed via an API call. An example of how to do this is below, but will not be used here. For this project, I collected the data in a separate notebook, saved, and uploaded to GitHub so others can use it easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# buoy_id = \"46025\" # Santa Monica Bay, CA\n",
    "# year = \"2023\"\n",
    "# url = f\"https://www.ndbc.noaa.gov/view_text_file.php?filename={buoy_id}h{year}.txt.gz&dir=data/historical/stdmet/\"\n",
    "# df = pd.read_csv(url, delim_whitespace=True, skiprows=[1])  # first row is metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://raw.githubusercontent.com/colinbeyers/GEOL558-submissions/refs/heads/main/module_3/NDBC_46025-wave_params-2023.csv'\n",
    "df = pd.read_csv(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset and buoy were chosen for a couple reasons:\n",
    "- It has a wind speed measurement device\n",
    "- It has WMO (World Meteorological Organization) seastate labels, which are used to classify seastate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The features in this dataset are:\n",
    "- YY/MM/DD/hh/mm: The year, month, day, hour, and minute of the observation\n",
    "- WDIR/WSPD: wind direction and speed\n",
    "- GST: gust speed\n",
    "- WVHT: wave height\n",
    "- DPD/APD: dominant and average period\n",
    "- MWD: mean wave direction\n",
    "- PRES: atmospheric pressure\n",
    "- ATMP/WTMP: atmospheric and water temperatures\n",
    "- DEWP: dew point\n",
    "- VIS: visibility\n",
    "- TIDE: tide height\n",
    "- WMOS: SMO sea state classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's a couple things I want to do with this data:\n",
    "1) Drop variables that do not contribute to sea state: WDIR, MWD, GST, PRES, ATMP/WTMP, DEWP, VIS, and TIDE. I am unsure which out of DPD and APD are considered for sea state classification (WMO and NOAA's websites doesn't make this clear).\n",
    "2) Rename variables to be more clear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['WDIR', 'MWD', 'GST', 'PRES', 'ATMP', 'WTMP', 'DEWP', 'VIS', 'TIDE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.rename(columns={\n",
    "    'WDIR': 'wind_dir',\n",
    "    'WSPD': 'wind_speed',\n",
    "    'WVHT': 'wave_height',\n",
    "    'DPD': 'dominant_period',\n",
    "    'APD': 'avg_period',\n",
    "    'WMOS': 'WMO_seastate'\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a lot of observations to work with here, however the \"max\" for each feature shows that many have a max of 99. This is likely used to express bad/missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['wave_height']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, `wave_height` only has actual observations every few rows. For all features, we should remove these so that they don't skew our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[~df[['avg_period', 'dominant_period', 'wave_height', 'wind_speed']].isin([99]).any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have WAY less observations now, but there are still well over 10,000, so this dataset should still work just fine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last preprocessing we should do is parse the `YY`/`MM`/`DD`/`hh`/`mm` columns to a single `time` datetime object column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['time'] = pd.to_datetime(df[['#YY', 'MM', 'DD', 'hh', 'mm']].astype(str).agg('-'.join, axis=1), format='%Y-%m-%d-%H-%M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['#YY', 'MM', 'DD', 'hh', 'mm'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['time'] + [col for col in df.columns if col != 'time']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the dataset does not have the lat/lon, NOAA's website labels them as: 33.755N 119.045W."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lat = 33.755\n",
    "lon = -119.045\n",
    "\n",
    "fig, ax = pyp.subplots(figsize=(8, 8), \n",
    "                       subplot_kw={'projection': ccrs.PlateCarree()})\n",
    "ax.set_extent([lon-2, lon+2, lat-2, lat+2])\n",
    "\n",
    "# features\n",
    "ax.coastlines(resolution='10m', color='black')\n",
    "ax.gridlines(draw_labels=True, zorder = 1)\n",
    "ax.add_feature(cfeature.OCEAN, facecolor='lightblue')\n",
    "ax.add_feature(cfeature.LAND, facecolor='wheat')\n",
    "\n",
    "# buoy location\n",
    "ax.scatter(lon, lat, \n",
    "           color='red', marker='^', edgecolor='black', \n",
    "           s=100, label='NDBC Buoy', zorder=2)\n",
    "ax.text(lon, lat-0.2, '46025', ha='center', fontsize=12, color='black')\n",
    "\n",
    "# title\n",
    "ax.set_title('Station Location')\n",
    "ax.legend(loc='upper right')\n",
    "\n",
    "pyp.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This area is known as the Santa Barbara Channel. It is relatively shallow and sheltered. Malibu, a popular surfing location, is located directly above and a little to the right if the buoy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WMO Code distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 10 WMO seastate codes:\n",
    "- 0: Calm (glassy)\n",
    "- 1: Calm (rippled)\n",
    "- 2: Smooth (wavelets)\n",
    "- 3: Slight\n",
    "- 4: Moderate\n",
    "- 5: Rough\n",
    "- 6: Very rough\n",
    "- 7: High\n",
    "- 8: Very high\n",
    "- 9: Phenomenal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the occurrences of each sea state\n",
    "seastate_counts = df['WMO_seastate'].value_counts()\n",
    "\n",
    "# Plot the bar graph\n",
    "pyp.figure(figsize=(10, 6))\n",
    "seastate_counts.plot(kind='bar', color='skyblue')\n",
    "pyp.title('Distribution of WMO Seastates')\n",
    "pyp.xlabel('Sea State')\n",
    "pyp.ylabel('Frequency')\n",
    "pyp.xticks(rotation=45)\n",
    "pyp.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset does not represent every code. The higher codes (6-9) are exceedingly rare and only every really observed in the Southern Ocean, North Atlantic, and North Pacific. It is unsurprising that in this sheltered channel, they are not observed. The 'calm' label could be 0 or 1, but I think it's probably 1. A truly glassy sea is very rare to observe; the NOAA documentations suggests that 'glassy' is used as the label for code 0. The most common code is 'sight', emphasizing the relatively calm/smooth waves that are usually found in this region."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = df[['wave_height', \n",
    "                  'dominant_period', \n",
    "                  'avg_period', \n",
    "                  'wind_speed']].corr()\n",
    "\n",
    "# Plot the heatmap\n",
    "pyp.figure(figsize=(8, 6))\n",
    "sbn.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt='.2f', cbar=True)\n",
    "pyp.title('Correlation Matrix Heatmap')\n",
    "pyp.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unsurprisingly, the wind speed and wave height are somewhat positively correlated. Stronger winds = higher waves. There is also a somewhat positive correlation between wave height and average period. This shows that longer period swell waves tend to be higher than shorter waves. There is a very weak correlation between the dominant period and the wave height."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Box and Whisker Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['wave_height', \n",
    "            'avg_period', \n",
    "            'dominant_period', \n",
    "            'wind_speed']\n",
    "\n",
    "fig, axes = pyp.subplots(nrows=1, ncols=len(features), figsize=(15, 6))\n",
    "\n",
    "for ax, feature in zip(axes, features):\n",
    "    sbn.boxplot(data=df[feature], ax=ax)\n",
    "    ax.set_title(feature)\n",
    "    ax.set_xlabel('')\n",
    "    ax.set_ylabel('')\n",
    "\n",
    "fig.suptitle('Boxplot of Numeric Features', fontsize=16)\n",
    "\n",
    "pyp.tight_layout()\n",
    "pyp.subplots_adjust(top=0.85)\n",
    "\n",
    "pyp.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Wave Height: wave heights in this region are relatively small for the entire year, averaging only 1m. Even a maximum of 6m is small compared to other locations not so protected on the California coast.\n",
    "- Average Period: the average period shows that most of the time, the seastate is dominated by locally generated wind waves. The maximum values >10s are likely when winds are quiet and swell is present.\n",
    "- Dominant Period: this shows what period of wave has the most energy when the observation was taken. Most dominant periods are >10s, which are swell waves (what people surf). This supports Malibu being such a great surfing location! Although the average period is >10s, swell waves have more energy than wind waves, so their imprint on the dominant period is more visible.\n",
    "- Wind Speed: average wind speeds in this area are relatively calm, but have the potential to be large (>10m/s) during storms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of these factors combined support what we see in the WMO labels - there are many instances of calmer seas. Calmer seas does not necessarily mean that the waves can't be powerful for surfing though! It just means that the waves are more organized and less \"rough\" (aggressive), which leads to a more pleasant beach and surfing experience."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wave Height Time Series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just to visualize how wave height varies with time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyp.figure(figsize=(12, 6))\n",
    "pyp.plot(df['time'], df['wave_height'], \n",
    "         color='cornflowerblue', label='Smoothed Significant Wave Height')\n",
    "pyp.title('Time Series of Significant Wave Height')\n",
    "pyp.xlabel('Date')\n",
    "pyp.ylabel('Wave Height (m)')\n",
    "pyp.xticks(rotation=45)\n",
    "pyp.grid(True, linestyle='--', alpha=0.5)\n",
    "pyp.legend()\n",
    "pyp.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before training a model, we need to do a few things:\n",
    "- Standardize the data (so no feature dominates the model based on scale)\n",
    "- Split the features and target variables\n",
    "- Encode the WMO codes to numbers\n",
    "- Split the data for testing and training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Normalize the features using StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "features = ['wind_speed', 'wave_height', 'avg_period']\n",
    "df[features] = scaler.fit_transform(df[features])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Encode the target variable `WMO_seastate`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'WMO_seastate'\n",
    "label_encoder = LabelEncoder()\n",
    "df[target] = label_encoder.fit_transform(df[target])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_mapping = dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))\n",
    "print(label_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Split the data temporally (the data is sorted by time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.8 * len(df))  # 80% for training, 20% for testing\n",
    "train_data = df[:train_size]\n",
    "test_data = df[train_size:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_data[features]\n",
    "y_train = train_data[target]\n",
    "X_test = test_data[features]\n",
    "y_test = test_data[target]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistical Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can first throw our data into the least complex model and see what happens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 4 evaluation metrics I will be using are:\n",
    "- **Precision**: measures the proportion of correctly predicted instances among all instances classified as a given category, indicating how many of the positive predictions were actually correct.  \n",
    "- **Recall**: measures the proportion of correctly predicted instances out of all actual instances in that category, reflecting the modelâ€™s ability to identify all relevant cases.  \n",
    "- **F1 Score**: the harmonic mean of precision and recall, providing a balanced metric when both false positives and false negatives are important.  \n",
    "- **Confusion Matrix**: a table that summarizes the number of correct and incorrect predictions for each class, helping to visualize the model's performance across categories.  \n",
    "\n",
    "A **weighted score** calculates the overall metric by averaging individual class scores, weighted by the number of true instances in each class, ensuring that larger classes contribute proportionally more to the final score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Precision (weighted): {precision:.3f}\")\n",
    "print(f\"Recall (weighted): {recall:.3f}\")\n",
    "print(f\"F1 Score (weighted): {f1:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Precision: out of all the instances the model predicted a specific sea state, 77.2% of those predictions were actually correct.\n",
    "- Recall: This shows that the model was able to correctly identify 78.7% of all actual sea state instances across all classes.\n",
    "- F1 Score: This suggests that the model's performance is reasonably balanced in both identifying true positives and avoiding false positives, with a slight favor towards recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "pyp.figure(figsize=(8, 6))\n",
    "sbn.heatmap(cm, annot=True, fmt='g', cmap='Blues', xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)\n",
    "pyp.title('Confusion Matrix')\n",
    "pyp.xlabel('Predicted')\n",
    "pyp.ylabel('Actual')\n",
    "pyp.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The model performs well for CALM, MODERATE, SLIGHT, and SMOOTH, with high accuracy.\n",
    "- The main confusion arises between CALM and SLIGHT, and SLIGHT with MODERATE.\n",
    "- ROUGH is predicted with lower accuracy, possibly due to its relatively small representation in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are not really parameters to tune for this type of model, so I will try 2 more complex models: Gradient Boosting and Support Vector Machine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will just use the default parameters first. This is an ensemble method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_model = GradientBoostingClassifier()\n",
    "gb_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_gb = gb_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_gb = precision_score(y_test, y_pred_gb, average='weighted')\n",
    "recall_gb = recall_score(y_test, y_pred_gb, average='weighted')\n",
    "f1_gb = f1_score(y_test, y_pred_gb, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Precision (weighted): {precision_gb}\")\n",
    "print(f\"Recall (weighted): {recall_gb}\")\n",
    "print(f\"F1 Score (weighted): {f1_gb}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Gradient Boosting model performs well with a precision of 0.856, recall of 0.853, and an F1 score of 0.845, indicating a strong balance between correctly identifying sea states and minimizing both false positives and false negatives. These results suggest the model effectively classifies the target variable, generalizing well across the different seastates. It performs better than the Logistical Classification model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_gb = confusion_matrix(y_test, y_pred_gb)\n",
    "\n",
    "pyp.figure(figsize=(8, 6))\n",
    "sbn.heatmap(cm_gb, annot=True, fmt='g', cmap='Blues', xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)\n",
    "pyp.title('Gradient Boosting - Confusion Matrix')\n",
    "pyp.ylabel('True Label')\n",
    "pyp.xlabel('Predicted Label')\n",
    "pyp.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This modeled performed better, but still struggles in similar areas as the Linear Classifier. This model is still getting confused between slight and moderate and between slight and smooth."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The default hyperparameters are unlikely to be the best. I will search across the most influential hyperparameters:\n",
    "\n",
    "- `n_estimators`: The number of boosting stages (trees) to build. More trees can improve performance but may lead to overfitting.\n",
    "- `learning_rate`: The step size for updating weights in each boosting iteration. Lower values make learning more gradual and reduce the risk of overfitting.\n",
    "- `max_depth`: The maximum depth of each decision tree. Deeper trees capture more complex patterns but increase the risk of overfitting.\n",
    "- `min_samples_split`: The minimum number of samples required to split a node. Higher values make the model more conservative, preventing overfitting.\n",
    "\n",
    "I will perform a **grid search** to find the optimal combination of these parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**WARNING**: this cell make take a while to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "gb_model = GradientBoostingClassifier()\n",
    "\n",
    "# Define the hyperparameters for grid search\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "# Set up the GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=gb_model, \n",
    "                           param_grid=param_grid, \n",
    "                           cv=5, \n",
    "                           scoring='accuracy', \n",
    "                           n_jobs=-1, \n",
    "                           verbose=2)\n",
    "\n",
    "# Fit the model with grid search\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters found by GridSearchCV\n",
    "best_params = grid_search.best_params_\n",
    "print(f\"Best hyperparameters: {best_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = grid_search.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "cm = confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Precision (weighted): {precision}\")\n",
    "print(f\"Recall (weighted): {recall}\")\n",
    "print(f\"F1 Score (weighted): {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyp.figure(figsize=(8, 6))\n",
    "sbn.heatmap(cm, annot=True, fmt='g', cmap='Blues', xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)\n",
    "pyp.title('Confusion Matrix')\n",
    "pyp.xlabel('Predicted')\n",
    "pyp.ylabel('Actual')\n",
    "pyp.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machine (SVM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even with hyperparameter tuning, the GB model still is having problems with the same categories. I'll finally try an SVM classification model. This model might be advantageous because of it's unique kernels which helps it handle non-linear data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_model = SVC(kernel='linear')\n",
    "svm_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = svm_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "cm = confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Precision (weighted):\", precision)\n",
    "print(\"Recall (weighted):\", recall)\n",
    "print(\"F1 Score (weighted):\", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyp.figure(figsize=(8, 6))\n",
    "sbn.heatmap(cm, annot=True, fmt='g', cmap='Blues', xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)\n",
    "pyp.title('Confusion Matrix')\n",
    "pyp.xlabel('Predicted')\n",
    "pyp.ylabel('Actual')\n",
    "pyp.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, the default parameters are likely not ideal. I will test the following parameters:\n",
    "\n",
    "- `kernel`: Defines the function used to map data into a higher-dimensional space. Common options include `'linear'`, `'poly'`, `'rbf'`, and `'sigmoid'`. The choice affects the model's ability to capture complex patterns.\n",
    "- `C`: The regularization parameter, which controls the trade-off between achieving a low error and maintaining a simple decision boundary. Higher values make the model more sensitive to training data.\n",
    "- `gamma`: Controls the influence of a single training example. Higher values lead to more complex decision boundaries, while lower values create a smoother model.\n",
    "\n",
    "I will perform a **grid search** to find the optimal combination of these parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameter grid for grid search\n",
    "param_grid = {\n",
    "    'kernel': ['linear', 'rbf', 'poly'],\n",
    "    'C': [0.1, 1, 10],\n",
    "    'gamma': ['scale', 'auto', 0.1, 1]\n",
    "}\n",
    "\n",
    "# Initialize the SVC model\n",
    "svc = SVC()\n",
    "\n",
    "# Set up GridSearchCV with 5-fold cross-validation\n",
    "grid_search = GridSearchCV(svc, param_grid, cv=5, scoring='accuracy', n_jobs=-1, verbose=1)\n",
    "\n",
    "# Fit the grid search to the data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters from the grid search\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Best parameters found by GridSearchCV:\", best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = grid_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "cm = confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Precision (weighted):\", precision)\n",
    "print(\"Recall (weighted):\", recall)\n",
    "print(\"F1 Score (weighted):\", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyp.figure(figsize=(8, 6))\n",
    "sbn.heatmap(cm_gb, annot=True, fmt='g', cmap='Blues', xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)\n",
    "pyp.title('Gradient Boosting - Confusion Matrix')\n",
    "pyp.ylabel('True Label')\n",
    "pyp.xlabel('Predicted Label')\n",
    "pyp.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we're seeing the same problem persist. It is likely not the model or the parameters, but how I'm choosing training and testing data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Which method did you like the most?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think the GB and SVM are the most promising. I liked them because they had hyperparameters that I could tune to fit the needs of my data better. Each also had a distinct advantage: GB is an ensemble approach, and SVM has custom kernels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Which method did you like the least?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I like logistical regression the least. While it was simple to set up and really fast, there are no hyperparameters to tune so it was hard to improve results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How did you score these supervised models?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I use precision, recall, F1, and confusion matrices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Did the output align with your geologic (oceanographic) understanding?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think each models confusion between slight and moderate is understandable from an oceanographic perspective. While rough seas and extremely calm seas are likely east to characterize because they have distinct characteristics that define them, the \"middle-of-the-road\" seastates like moderate and slight are probably less distinct, which is supported by how each model struggled with them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Did you hyperparameter tune? Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, I did hyperparameter tuning for the GB and SVM models. The default settings (or any initial settings I may guess) for either model are very unlikely to be the ones that provide the best fit. Grid searching is a good method for systematically testing for different hyperparameters without having to run them all manually, which is why I used it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How did you split your data? Why does that make sense for this dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I chose to do a temporal split where I took the first 80% of data (in time order) and used it for training and took the last 20% and set it aside for testing. This was to ensure that my testing set was as independent as possible from my training set and to avoid autocorrelation issues. As with most geophysical data, observations close in time to one another are not independent---they are related to some decorrelation scale. A random split very likely would have put data that is closely correlated in both the training and testing set. Also, using this method of temporal splitting also assessed how my model was able to generalize. As I had 1 year of data, the last 20% of data was from the winter months of 2023. Wave conditions are not the same all year round (and are highly seasonal), so using data from a different season assessed how well the model was able to generalize to new conditions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What did you want to learn more about?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think the problem I was having with the moderate and slight categories has to do with the fact that my labels are highly imbalanced. I think for my final version, I want to play around with either a) finding ways to rebalance my datasets; or b) finding a way to penalize models more during training when they misclassify a minority label/award them for classifying one correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Did you pre-process your data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes. I had to:\n",
    "- Remove unused features\n",
    "- Rename columns to make them more intuitive\n",
    "- Parse the native timestamps to datetime objects\n",
    "- Remove bad data\n",
    "- Scale my features\n",
    "- Encode my labels to a number\n",
    "- Split my data into training and testing sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do all models require pre-processing? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In my opinion, yes---all models require some level of preprocessing. On a higher level, things like missing data (NaNs or set to 99, like in this case) or data that is not formatted in an intuitive way (like the timestamps in this dataset) can make the modeling process needlessly confusing, can drastically skew results, or can altogether make a model fail. More specifically, things like thoughtful train-test splitting and scaling features are subtle steps but ensure that a model is robust and not misleading or inaccurate."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
